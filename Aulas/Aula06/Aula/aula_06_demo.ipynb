{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39513f2d7e6e0fb6",
   "metadata": {},
   "source": [
    "## Configuração e Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc2c32b982ef6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- GERAÇÃO DE DADOS ---\n",
    "# 200 Pacientes, 1000 Genes. Apenas os primeiros 20 são informativos.\n",
    "X, y = make_classification(\n",
    "    n_samples=200, n_features=1000, n_informative=20,\n",
    "    n_redundant=0, n_repeated=0, n_classes=2,\n",
    "    random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "gene_names = [f\"Gene_{i}\" for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=gene_names)\n",
    "\n",
    "# Split Treino/Teste (Fundamental para validar a seleção)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Escalar (Obrigatório para alguns métodos)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Dados gerados: {X_train.shape} (Treino).\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bd226e25eab3da",
   "metadata": {},
   "source": [
    "### 2. Filter Method: Seleção Estatística Univariada (SelectKBest)\n",
    "\n",
    "Os métodos de filtro (**Filter Methods**) são os mais rápidos e simples. Eles avaliam cada variável individualmente em relação à variável alvo (`y`), calculam uma nota (score) e selecionam apenas as melhores.\n",
    "\n",
    "Neste exemplo, usamos o **SelectKBest**, que mantém as $k$ variáveis com os maiores scores estatísticos.\n",
    "\n",
    "**Passo a passo do código:**\n",
    "1.  **Definição da Métrica (`score_func`):** Usamos o `f_classif`, que realiza um teste ANOVA (Análise de Variância). Ele verifica se a média do gene é significativamente diferente entre os pacientes doentes e saudáveis.\n",
    "2.  **Seleção (`k`):** Definimos `k=50` para manter apenas os 50 genes com maior pontuação.\n",
    "3.  **Visualização:** O gráfico de barras mostra o \"F-Score\" de cada gene.\n",
    "\n",
    "---\n",
    "\n",
    "#### Como variar os parâmetros (Experimente!):\n",
    "\n",
    "* **`k` (Número de Features):**\n",
    "    * Tente alterar `k=50` para `k=10` ou `k=200`.\n",
    "    * Se `k='all'`, ele retorna as notas de todas as variáveis sem filtrar nada (bom para análise exploratória).\n",
    "\n",
    "* **`score_func` (Função de Pontuação):**\n",
    "    * **`f_classif`:** Usado aqui (Ideal para: *Input Numérico -> Target Categórico*).\n",
    "    * **`chi2`:** Use para dados não-negativos, como contagem de palavras (Ideal para: *Input Categórico -> Target Categórico*).\n",
    "    * **`mutual_info_classif`:** Captura relações não-lineares (mais pesado computacionalmente).\n",
    "    * **`f_regression`:** Se o seu problema fosse prever um número contínuo (Regressão), não uma classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffd3383e31c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Selecionar os top 50 genes baseados em estatística ANOVA (f_classif)\n",
    "selector_filter = SelectKBest(score_func=f_classif, k=50) # Outras score functions\n",
    "selector_filter.fit(X_train, y_train)\n",
    "\n",
    "# Quais foram escolhidos? (Máscara booleana)\n",
    "mask = selector_filter.get_support()\n",
    "selected_genes_filter = np.array(gene_names)[mask]\n",
    "\n",
    "print(\"--- FILTER METHOD ---\")\n",
    "print(f\"Top 5 genes selecionados: {selected_genes_filter[:5]}\")\n",
    "\n",
    "# Visualizando os Scores dos primeiros 100 genes\n",
    "scores = selector_filter.scores_\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(100), scores[:100])\n",
    "plt.title(\"Scores Estatísticos (ANOVA) - Primeiros 100 Genes\")\n",
    "plt.xlabel(\"Índice do Gene\")\n",
    "plt.ylabel(\"F-Score\")\n",
    "plt.show()\n",
    "print(\"Nota: O filtro detectou bem o pico nos primeiros 20 genes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768bf5326094f466",
   "metadata": {},
   "source": [
    "### 3. Wrapper Method: Eliminação Recursiva de Features (RFE)\n",
    "\n",
    "Os métodos Wrapper (\"Envelopados\") utilizam um modelo preditivo real para avaliar a importância das variáveis. O **RFE (Recursive Feature Elimination)** funciona como um processo de \"seleção natural\":\n",
    "\n",
    "1.  Treina o modelo com todas as variáveis.\n",
    "2.  Identifica as variáveis menos importantes (menores coeficientes ou *feature importance*).\n",
    "3.  Remove (\"poda\") essas variáveis do conjunto.\n",
    "4.  Repete o processo até sobrar apenas o número desejado de variáveis.\n",
    "\n",
    "**Passo a passo do código:**\n",
    "1.  **O Estimador Base (`estimator`):** Definimos uma `LogisticRegression`. É este modelo que vai decidir quem fica e quem sai a cada rodada. Aumentamos `max_iter` para garantir que modelo irá convergir mesmo com muitos dados.\n",
    "2.  **O Seletor RFE:** Configuramos o RFE para usar a Regressão Logística e parar quando restarem apenas **20 genes**.\n",
    "3.  **O Passo (`step=0.1`):** Para acelerar o processo (já que temos 1000 colunas), configuramos para remover **10%** das piores features a cada iteração, em vez de remover apenas uma por vez.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Como variar os parâmetros (Experimente!):\n",
    "\n",
    "* **`estimator` (O Juiz):**\n",
    "    * Tente trocar `LogisticRegression` por `RandomForestClassifier()` ou `LinearSVC()`.\n",
    "    * *Nota:* O modelo escolhido **precisa** ter os atributos `coef_` ou `feature_importances_` (modelos como KNN ou Naive Bayes não funcionam aqui).\n",
    "\n",
    "* **`step` (Velocidade vs. Precisão):**\n",
    "    * **`step=1`:** Remove 1 variável por vez. É o mais preciso, mas extremamente lento (treinará o modelo 980 vezes!).\n",
    "    * **`step=0.1` a `0.5`:** Remove blocos de variáveis (10% a 50%). Muito mais rápido, ideal para Big Data.\n",
    "\n",
    "* **`n_features_to_select` (A Meta):**\n",
    "    * Define quantos \"sobreviventes\" você quer no final. Se você não souber o número ideal, existe uma variação chamada **RFECV** que usa validação cruzada para descobrir o número ótimo automaticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad0208f5b2ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Usaremos Regressão Logística como base\n",
    "estimator = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "\n",
    "# RFE: Quero que sobrem apenas 20 genes\n",
    "# step=0.1 significa remover 10% das features a cada iteração (para ser rápido)\n",
    "rfe = RFE(estimator=estimator, n_features_to_select=20, step=0.1)\n",
    "#rfe_cv = RFECV(estimator=estimator, step=0.1)\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "\n",
    "selected_genes_rfe = np.array(gene_names)[rfe.support_]\n",
    "\n",
    "print(\"--- WRAPPER METHOD (RFE) ---\")\n",
    "print(f\"Genes selecionados pelo RFE: {selected_genes_rfe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc4c923d66c6b1b",
   "metadata": {},
   "source": [
    "### 4. Embedded Method: Seleção Baseada em Árvores (Random Forest)\n",
    "\n",
    "Os métodos embarcados (**Embedded Methods**) realizam a seleção de atributos como parte integrante do processo de treinamento do modelo. Eles são mais eficientes que os Wrappers e mais precisos que os Filters.\n",
    "\n",
    "O **Random Forest** é o exemplo clássico. Ele constrói centenas de árvores de decisão.\n",
    "* Em cada nó de cada árvore, o algoritmo escolhe a variável que melhor separa as classes (reduz a impureza/Gini).\n",
    "* Se uma variável é escolhida muitas vezes e no topo das árvores, ela ganha uma **Importância** alta.\n",
    "* Se uma variável é ruído, ela quase nunca é escolhida, ficando com importância próxima de zero.\n",
    "\n",
    "\n",
    "\n",
    "**Passo a passo do código:**\n",
    "1.  **Treinamento:** Instanciamos e treinamos o `RandomForestClassifier`. Note que não usamos nenhuma função de seleção separada (como `RFE` ou `SelectKBest`). O próprio `.fit()` calcula tudo.\n",
    "2.  **Extração (`feature_importances_`):** O modelo possui um atributo interno que guarda a soma da redução de impureza de cada variável.\n",
    "3.  **Ordenação (`argsort`):** Como o array de importâncias vem na ordem original das colunas (Gene_0, Gene_1...), usamos o `argsort` do NumPy com `[::-1]` para ordenar do maior para o menor e descobrir os \"campeões\".\n",
    "\n",
    "---\n",
    "\n",
    "#### Como variar os parâmetros (Experimente!):\n",
    "\n",
    "* **`n_estimators` (Número de Árvores):**\n",
    "    * O padrão é 100.\n",
    "    * Aumentar para `500` ou `1000` torna a estimativa da importância mais estável e confiável (reduz a variância da escolha), mas demora mais para treinar.\n",
    "\n",
    "* **`criterion` (Critério de Divisão):**\n",
    "    * **`gini`:** (Padrão) Mede a impureza de Gini. Geralmente mais rápido.\n",
    "    * **`entropy`:** Mede o Ganho de Informação. Às vezes seleciona atributos ligeiramente diferentes. Vale testar se a lista de genes muda.\n",
    "\n",
    "* **`max_depth` (Profundidade da Árvore):**\n",
    "    * Se você limitar a profundidade (ex: `max_depth=3`), força o modelo a escolher apenas as variáveis **super cruciais** (as que aparecem no topo), ignorando interações complexas e profundas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328de75894ecd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Treino\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Importâncias\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] # Ordenar decrescente\n",
    "\n",
    "print(\"--- EMBEDDED METHOD (Random Forest) ---\")\n",
    "print(\"Top 5 Genes mais importantes:\")\n",
    "for i in range(5):\n",
    "    print(f\"{gene_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
    "\n",
    "# Gráfico\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title(\"Feature Importance (Random Forest) - Top 30\")\n",
    "plt.bar(range(30), importances[indices[:30]], align=\"center\")\n",
    "plt.xticks(range(30), [gene_names[i] for i in indices[:30]], rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ea508fcbc841e",
   "metadata": {},
   "source": [
    "### 5. Extração de Atributos: PCA (Principal Component Analysis)\n",
    "\n",
    "Diferente dos métodos anteriores que **descartam** variáveis (Seleção), o PCA **mantém todas elas**, mas de uma forma compactada. Ele realiza uma transformação matemática (álgebra linear) para criar \"novos eixos\" que resumem a informação.\n",
    "\n",
    "Imagine que temos 1000 genes.\n",
    "* **Seleção (RFE/Lasso):** Escolhe 2 genes e joga 998 no lixo.\n",
    "* **Extração (PCA):** Cria 2 \"Super-Variáveis\" (PC1 e PC2). O PC1 é uma mistura ponderada de todos os 1000 genes.\n",
    "\n",
    "**Passo a passo do código:**\n",
    "1.  **Definição (`n_components=2`):** Pedimos ao PCA para reduzir o mundo de 1000 dimensões para apenas 2. Por que 2? Porque queremos visualizar em uma tela (eixo X e Y).\n",
    "2.  **O Input (`X_train_scaled`):**\n",
    "    *  **Atenção Crítica:** O PCA é extremamente sensível à escala dos dados. Se você não usar o `StandardScaler` antes, o PCA vai considerar que variáveis com números grandes (ex: salário) são mais importantes que variáveis com números pequenos (ex: idade). **Sempre padronize antes do PCA.**\n",
    "3.  **A Transformação:** O método `.fit_transform()` calcula a matriz de rotação e já aplica a projeção, retornando as coordenadas dos pacientes nesse novo mundo 2D.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Como variar os parâmetros (Experimente!):\n",
    "\n",
    "* **`n_components` (O Grau de Compressão):**\n",
    "    * **Número Inteiro (ex: `2`, `3`, `50`):** Define exatamente quantas colunas você quer na saída. Use 2 ou 3 para gráficos.\n",
    "    * **Número Decimal (ex: `0.95`, `0.99`):** Isso muda o comportamento! Em vez de pedir \"X colunas\", você diz: *\"Mantenha colunas suficientes para preservar **95%** da informação original\"*. O algoritmo decide sozinho se precisa de 10, 50 ou 200 componentes. É muito usado em Machine Learning real.\n",
    "\n",
    "* **`svd_solver`:**\n",
    "    * Para datasets gigantescos, você pode usar `svd_solver='randomized'`, que faz uma aproximação estatística muito mais rápida do que o cálculo exato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f038234d413eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train_scaled) # PCA sempre nos dados escalados!\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y_train, palette='viridis', style=y_train, s=100)\n",
    "plt.title(\"PCA: Projeção 2D dos Pacientes\")\n",
    "plt.xlabel(\"PC1 (Maior Variância)\")\n",
    "plt.ylabel(\"PC2 (Segunda Maior)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f825397829072ca8",
   "metadata": {},
   "source": [
    "### Determinando o Número Ideal de Componentes\n",
    "\n",
    "Antes de transformarmos os dados, precisamos responder à pergunta de ouro: **\"Quantos componentes devemos manter?\"**\n",
    "\n",
    "Não precisamos chutar um número. Podemos calcular exatamente quantos eixos são necessários para preservar uma certa porcentagem da informação original (geralmente 90%, 95% ou 99%).\n",
    "\n",
    "Isso é feito analisando a **Variância Explicada Acumulada**:\n",
    "1.  O PC1 explica X% (ex: 40%).\n",
    "2.  O PC1 + PC2 explicam Y% (ex: 40% + 20% = 60%).\n",
    "3.  Continuamos somando até atingir nossa meta (ex: 95%).\n",
    "\n",
    "No código abaixo, vamos gerar o **Scree Plot** (Gráfico de Cotovelo) para visualizar essa soma e descobrir o ponto de corte ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a63ccb0b5776853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Instanciar PCA sem limitar o número de componentes\n",
    "# (Ou limitando ao máximo possível, que é o min(n_samples, n_features))\n",
    "pca_full = PCA(n_components=None)\n",
    "pca_full.fit(X_train_scaled)\n",
    "\n",
    "# 2. Calcular a Variância Acumulada\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# 3. Determinar matematicamente o corte (Ex: 95%)\n",
    "limite_desejado = 0.95\n",
    "n_components_ideal = np.argmax(cumulative_variance >= limite_desejado) + 1\n",
    "\n",
    "print(f\"Para preservar {limite_desejado*100}% da informação, precisamos de {n_components_ideal} componentes.\")\n",
    "\n",
    "# 4. Plotar o Scree Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cumulative_variance, marker='o', linestyle='--', color='b')\n",
    "plt.axhline(y=limite_desejado, color='r', linestyle='-', label=f'Corte de {limite_desejado*100}%')\n",
    "plt.axvline(x=n_components_ideal-1, color='r', linestyle='--')\n",
    "plt.xlabel('Número de Componentes')\n",
    "plt.ylabel('Variância Explicada Acumulada')\n",
    "plt.title('Scree Plot: Quantos componentes guardar?')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b772f6ac409cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Em vez de escolher '2' ou '10', escolhemos '0.95' (95%)\n",
    "pca_auto = PCA(n_components=0.95)\n",
    "X_pca_auto = pca_auto.fit_transform(X_train_scaled)\n",
    "\n",
    "print(f\"O PCA selecionou automaticamente {pca_auto.n_components_} componentes para atingir 95% de variância.\")\n",
    "print(f\"Shape original: {X_train_scaled.shape}\")\n",
    "print(f\"Shape reduzido: {X_pca_auto.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4618f51a67e388f",
   "metadata": {},
   "source": [
    "### Visualizando a Escolha do Número de Componentes (Zoom nos Top 30)\n",
    "\n",
    "Muitas vezes, visualizar todos os componentes (ex: 1000) torna o gráfico ilegível. Por isso, focamos apenas nos primeiros (Top 30), onde a maior parte da ação acontece.\n",
    "\n",
    "O código abaixo gera dois gráficos fundamentais para a sua tomada de decisão:\n",
    "\n",
    "1.  **Gráfico da Esquerda (Individual - O \"Cotovelo\"):**\n",
    "    * Mostra quanto cada componente contribui isoladamente.\n",
    "    * **O que procurar:** O ponto onde a curva deixa de cair bruscamente e passa a ficar \"plana\". Isso indica que os componentes seguintes são apenas ruído.\n",
    "\n",
    "2.  **Gráfico da Direita (Acumulada - A \"Meta\"):**\n",
    "    * Mostra a soma da informação retida.\n",
    "    * **O que procurar:** O ponto exato onde a linha laranja cruza a linha vermelha pontilhada (nossa meta de 95%).\n",
    "    * O código desenhará uma **linha verde** vertical indicando o número ideal ($k$) para essa meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53821d289cc1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Definindo um limite visual (ex: mostrar apenas os top 30 para não espremer o gráfico)\n",
    "limit = 30\n",
    "variance_ratio = pca_full.explained_variance_ratio_[:limit]\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)[:limit]\n",
    "\n",
    "# Configurando a figura com 2 gráficos lado a lado\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# --- GRÁFICO 1: O COTOVELO (Individual) ---\n",
    "# Mostra onde a informação \"despenca\"\n",
    "ax1.bar(range(1, len(variance_ratio)+1), variance_ratio, alpha=0.7, color='steelblue', label='Variância Individual')\n",
    "ax1.plot(range(1, len(variance_ratio)+1), variance_ratio, 'r.-') # Linha para destacar a queda\n",
    "ax1.set_title('Scree Plot: Onde está o Cotovelo?')\n",
    "ax1.set_xlabel('Componentes Principais')\n",
    "ax1.set_ylabel('Variância Explicada')\n",
    "ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "ax1.legend()\n",
    "\n",
    "# --- GRÁFICO 2: A SOMA (Acumulada) ---\n",
    "# Mostra quando atingimos a meta (ex: 90%)\n",
    "ax2.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='o', linestyle='-', color='darkorange', linewidth=2)\n",
    "ax2.set_title('Variância Acumulada: Quando parar?')\n",
    "ax2.set_xlabel('Componentes Principais')\n",
    "ax2.set_ylabel('Total de Informação (%)')\n",
    "\n",
    "# Linha de Corte (Threshold) de 95%\n",
    "threshold = 0.95\n",
    "ax2.axhline(y=threshold, color='red', linestyle='--', label=f'Meta de {threshold*100}%')\n",
    "\n",
    "# Encontrar o ponto exato onde cruzou 95%\n",
    "k_ideal = np.argmax(cumulative_variance >= threshold) + 1\n",
    "ax2.axvline(x=k_ideal, color='green', linestyle='--', label=f'Ideal: {k_ideal} Comps')\n",
    "\n",
    "ax2.legend()\n",
    "ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Conclusão Visual: O 'cotovelo' acontece cedo, mas para ter {threshold*100}% de precisão precisamos de {k_ideal} componentes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94e245268a1631",
   "metadata": {},
   "source": [
    "### Cálculo Matemático do Corte (Sem \"Olhômetro\")\n",
    "\n",
    "Embora o gráfico ajude a ter uma intuição, em ciência de dados precisamos de reprodutibilidade.\n",
    "\n",
    "Se não conseguirmos identificar visualmente o cotovelo, ou se quisermos automatizar o processo, usamos a seguinte lógica:\n",
    "\n",
    "1.  Definimos uma meta (Threshold), por exemplo, **0.95 (95%)**.\n",
    "2.  Percorremos a lista de variância acumulada.\n",
    "3.  Paramos no **primeiro componente** que fizer a soma ultrapassar 0.95.\n",
    "\n",
    "O código abaixo faz isso instantaneamente usando a função `np.argmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f75a41edbaca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Definimos nossa meta de qualidade (ex: 95% da informação original)\n",
    "target_variance = 0.95\n",
    "\n",
    "# 2. Calculamos a variância acumulada (Soma progressiva)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# 3. O \"Pulo do Gato\": np.argmax retorna o PRIMEIRO índice que satisfaz a condição\n",
    "# Somamos +1 porque índices em Python começam em 0 (o componente 1 é índice 0)\n",
    "n_components_ideal = np.argmax(cumulative_variance >= target_variance) + 1\n",
    "\n",
    "print(f\"--- CÁLCULO AUTOMÁTICO ---\")\n",
    "print(f\"Para explicar {target_variance*100}% da variância, precisamos de: {n_components_ideal} componentes.\")\n",
    "print(f\"Variância real atingida com {n_components_ideal} componentes: {cumulative_variance[n_components_ideal-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10edead2eb8546a",
   "metadata": {},
   "source": [
    "### Definindo a Meta: Otimização Automática\n",
    "\n",
    "Em vez de \"chutar\" 90% ou 95%, podemos perguntar ao computador:\n",
    "*\"Qual é o número de componentes que faz o meu modelo de Câncer ter a maior acurácia possível?\"*\n",
    "\n",
    "Isso transforma o número de componentes em um **Hiperparâmetro** a ser otimizado.\n",
    "\n",
    "O código abaixo usa o `GridSearchCV` para testar várias quantidades de componentes e nos dizer qual funcionou melhor para a classificação final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38470007be9bcb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1. Criar um \"Tubo\" (Pipeline) que conecta o PCA ao Modelo\n",
    "pca = PCA()\n",
    "logistic = LogisticRegression(max_iter=1000)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "\n",
    "# 2. Definir os testes: Vamos testar 2, 5, 10, 15, 20 componentes... até 30\n",
    "param_grid = {\n",
    "    'pca__n_components': [2, 5, 10, 15, 20, 25, 30]\n",
    "}\n",
    "\n",
    "# 3. Rodar a busca (Grid Search) com Validação Cruzada (cv=5)\n",
    "search = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')\n",
    "search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Melhor número de componentes calculado:\", search.best_params_)\n",
    "print(\"Acurácia atingida:\", search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
