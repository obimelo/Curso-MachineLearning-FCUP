{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e2a086c83bd36bf",
   "metadata": {},
   "source": [
    "# Passo 1: Gerando o Dataset\n",
    "\n",
    "Vamos começar criando um dataset sintético para ilustrar o problema. Usaremos a função `make_blobs` para criar grupos compactos, que é o cenário ideal para o algoritmo K-Means.\n",
    "\n",
    "Imagine que estes pontos representam clientes e as coordenadas são variáveis como \"Idade\" e \"Gasto\".\n",
    "\n",
    "### Parâmetros Importantes:\n",
    "* **`n_samples`**: O número total de observações (ex: 500 clientes).\n",
    "* **`centers`**: O número real de grupos que vamos gerar. Como estamos simulando um problema não supervisionado, o algoritmo não saberá esse número; nós usaremos isso apenas para verificar se ele acertou no final.\n",
    "* **`cluster_std`**: O desvio padrão dos clusters. Define o quão \"espalhados\" ou compactos eles são.\n",
    "* **`random_state`**: A semente para garantir que seus dados sejam iguais aos meus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2ea460e4dfb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Gerar dados simulados (Clusters compactos)\n",
    "X, y_true = make_blobs(n_samples=500, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Visualizar os dados \"crus\" (como o algoritmo vê)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20)\n",
    "plt.title(\"Dados Originais (Sem Rótulos)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91a8a1e519a2941",
   "metadata": {},
   "source": [
    "# Passo 2: Padronização dos Dados\n",
    "\n",
    "Para algoritmos baseados em distância (como K-Means), a escala das variáveis é crítica. Se uma variável varia entre 0-1000 e outra entre 0-1, a primeira dominará o cálculo da distância Euclidiana.\n",
    "\n",
    "Para resolver isso, usamos o `StandardScaler` (Z-score), transformando os dados para que tenham média 0 e desvio padrão 1.\n",
    "\n",
    "### Parâmetros Importantes:\n",
    "* **`fit_transform(X)`**: Calcula a média e o desvio padrão de cada coluna em `X` e aplica a transformação $z = \\frac{x - \\mu}{\\sigma}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7007d7b48e619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Média antes escala:\", np.mean(X))\n",
    "print(\"Desvio padrão antes escala:\", np.std(X))\n",
    "print(\"Média após escala:\", np.mean(X_scaled))\n",
    "print(\"Desvio padrão após escala:\", np.std(X_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5b1449445e701",
   "metadata": {},
   "source": [
    "# Passo 3: Escolhendo k (Método do Cotovelo)\n",
    "\n",
    "O K-Means exige que definamos o número de clusters ($k$) antes de rodar. Para descobrir o melhor $k$, usamos o Método do Cotovelo (Elbow Method).\n",
    "\n",
    "Rodamos o algoritmo para vários valores de $k$ e observamos a **Inércia (SSE)**. O objetivo é encontrar o ponto onde o ganho marginal diminui drasticamente (o \"cotovelo\").\n",
    "\n",
    "### Parâmetros Importantes do `KMeans`:\n",
    "* **`n_clusters`**: O número de grupos ($k$) que estamos testando naquela iteração.\n",
    "* **`n_init`**: Quantas vezes o algoritmo vai rodar com sementes diferentes. A regra prática sugere executar várias inicializações e ficar com a menor SSE. O padrão do scikit-learn é 10.\n",
    "* **`random_state`**: Para reprodutibilidade.\n",
    "* **Atributo `inertia_`**: Retorna a soma dos erros ao quadrado (SSE) do modelo treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4d71b4a753a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "sse = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Plotar o gráfico do Cotovelo\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_range, sse, marker='o')\n",
    "plt.title('Método do Cotovelo (Elbow Method)')\n",
    "plt.xlabel('Número de clusters (k)')\n",
    "plt.ylabel('Inércia (SSE)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999eb21ae479f836",
   "metadata": {},
   "source": [
    "# Passo 4: Aplicando o K-Means\n",
    "\n",
    "Analisando o gráfico acima, devemos ver um \"cotovelo\" claro em $k=4$. A curva achata depois desse ponto. Vamos treinar o modelo final com esse valor.\n",
    "\n",
    "### Parâmetros Importantes:\n",
    "* **`n_clusters=4`**: O valor escolhido via método do cotovelo.\n",
    "* **`fit_predict(X)`**: Executa dois passos:\n",
    "    1.  Calcula os centróides (treina).\n",
    "    2.  Atribui cada ponto ao cluster mais próximo (prediz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3668e7fc1402e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando com o k ideal\n",
    "kmeans_final = KMeans(n_clusters=4, n_init=10, random_state=42)\n",
    "y_kmeans = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# Visualizando o resultado\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_kmeans, s=20, cmap='viridis')\n",
    "\n",
    "# Plotar os centróides\n",
    "centers = kmeans_final.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.7, label='Centróides')\n",
    "plt.title(\"Clusterização Final (k=4)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b537ae462149f6fe",
   "metadata": {},
   "source": [
    "# Passo 5: Avaliação com Silhouette Score\n",
    "\n",
    "Para confirmar se a partição é boa sem usar rótulos externos, usamos o **Silhouette Score**. Ele mede o quão similar um ponto é ao seu próprio cluster em comparação aos outros clusters.\n",
    "\n",
    "* Valor próximo de +1: Clusters bem definidos e separados.\n",
    "* Valor próximo de 0: Sobreposição de clusters.\n",
    "* Valor negativo: Pontos possivelmente no cluster errado.\n",
    "\n",
    "### Parâmetros Importantes:\n",
    "* **`silhouette_score(X, labels)`**: Recebe os dados e os rótulos preditos pelo modelo. Retorna a média de todos os pontos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c8f95847666ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "score = silhouette_score(X_scaled, y_kmeans)\n",
    "print(f\"Silhouette Score para k=4: {score:.3f}\")\n",
    "\n",
    "# Interpretação rápida\n",
    "if score > 0.5:\n",
    "    print(\"Resultado: Clusters bem separados e definidos.\")\n",
    "elif score > 0.2:\n",
    "    print(\"Resultado: Separação razoável.\")\n",
    "else:\n",
    "    print(\"Resultado: Clusters sobrepostos ou mal definidos.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
