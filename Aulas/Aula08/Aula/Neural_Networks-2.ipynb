{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OB_5KCAlSc9w"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF2EkfPJkhg8"
      },
      "source": [
        "# Entendendo o C√≥digo de Classifica√ß√£o de D√≠gitos (MNIST)\n",
        "\n",
        "Este bloco de c√≥digo demonstra como carregar uma imagem do famoso dataset MNIST e pass√°-la por uma Rede Neural simples (MLP) para prever qual d√≠gito est√° escrito √† m√£o.\n",
        "\n",
        "\n",
        "\n",
        "### 1. Prepara√ß√£o e Carregamento dos Dados\n",
        "* **`transform`**: Converte as imagens baixadas para o formato \"Tensor\" do PyTorch.\n",
        "* **`mnist_test`**: Baixa e carrega o conjunto de dados de teste (valida√ß√£o).\n",
        "* **`image, label`**: Extrai a primeira imagem do dataset (√≠ndice 0) e seu r√≥tulo real (a resposta correta).\n",
        "\n",
        "### 2. Achatamento da Imagem (Flattening)\n",
        "* As imagens originais t√™m 28x28 pixels. Redes neurais simples precisam de um vetor unidimensional (uma linha reta de dados) para funcionar.\n",
        "* O comando `view(1, 784)` \"achata\" a grade da imagem em um √∫nico vetor de 784 n√∫meros (28 * 28 = 784). O `1` no in√≠cio indica que estamos processando um lote (*batch*) de apenas 1 imagem.\n",
        "\n",
        "### 3. Constru√ß√£o da Rede Neural (Modelo)\n",
        "\n",
        "* **`nn.Sequential`**: Cria a rede empilhando camadas de forma sequencial.\n",
        "* **`nn.Linear(784, 128)`**: Camada de entrada. Recebe os 784 pixels e os conecta a 128 \"neur√¥nios\" ocultos.\n",
        "* **`nn.ReLU()`**: Fun√ß√£o de ativa√ß√£o. Adiciona n√£o-linearidade ao modelo, permitindo que ele aprenda padr√µes complexos.\n",
        "* **`nn.Linear(128, 10)`**: Camada de sa√≠da. Reduz os 128 sinais para 10 sa√≠das, representando as 10 classes poss√≠veis (os d√≠gitos de 0 a 9).\n",
        "\n",
        "### 4. Fazendo a Previs√£o (Infer√™ncia)\n",
        "* **`torch.no_grad()`**: Desativa o c√°lculo de gradientes (derivadas) para economizar mem√≥ria e processamento, j√° que estamos apenas testando, e n√£o treinando.\n",
        "* **`logits`**: A sa√≠da \"bruta\" do modelo com valores arbitr√°rios para cada classe.\n",
        "* **`torch.softmax(...)`**: Converte os valores brutos em probabilidades que somam 1 (100%).\n",
        "* **`torch.argmax(...)`**: Olha para as probabilidades e escolhe o √≠ndice com o maior valor. Esta √© a previs√£o final da rede!\n",
        "\n",
        "> **‚ö†Ô∏è Nota:** Como este modelo acabou de ser criado e ainda **n√£o foi treinado**, seus pesos internos s√£o completamente aleat√≥rios. A previs√£o inicial ser√° apenas um \"chute\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5z16UqtP--D",
        "outputId": "c970eb5e-0007-4d45-fbbb-f7a1c6b101bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n",
            "100.0%\n",
            "100.0%\n",
            "100.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- MLP MNIST CLASSIFICATION TEST ---\n",
            "Actual Label: 7\n",
            "Output Shape: torch.Size([1, 10])\n",
            "Predicted Class: 1\n",
            "Confidence in Prediction: 0.1209\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Digit Classification\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "image, label = mnist_test[0]\n",
        "\n",
        "flat_image = image.view(1, 784)\n",
        "\n",
        "mlp_model = nn.Sequential(\n",
        "    nn.Linear(784, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = mlp_model(flat_image)\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "    prediction = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "print(\"--- MLP MNIST CLASSIFICATION TEST ---\")\n",
        "print(f\"Actual Label: {label}\")\n",
        "print(f\"Output Shape: {logits.shape}\") # [1, 10]\n",
        "print(f\"Predicted Class: {prediction.item()}\")\n",
        "print(f\"Confidence in Prediction: {probabilities.max().item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7655KyPVnY_2"
      },
      "source": [
        "# Extraindo Mapas de Caracter√≠sticas (Feature Maps) com CNNs\n",
        "\n",
        "Este bloco demonstra o passo fundamental das Redes Neurais Convolucionais (CNNs): aplicar filtros matem√°ticos em uma imagem colorida (do dataset CIFAR-10) para extrair padr√µes visuais, como bordas, texturas e cores.\n",
        "\n",
        "### 1. Prepara√ß√£o dos Dados (Dataset CIFAR-10)\n",
        "* **`CIFAR10(...)`**: Baixa o dataset CIFAR-10. Diferente do MNIST (que √© preto e branco), o CIFAR-10 cont√©m imagens coloridas de 32x32 pixels divididas em 10 classes (avi√µes, carros, p√°ssaros, etc.).\n",
        "* **`image, label`**: Pega a primeira imagem de teste. No PyTorch, o formato dessa imagem ser√° `[3, 32, 32]` (3 canais de cor RGB, 32 pixels de altura, 32 pixels de largura).\n",
        "\n",
        "### 2. Preparando o \"Lote\" (Batching)\n",
        "* **`image.unsqueeze(0)`**: O PyTorch sempre espera processar imagens em lotes (*batches*), mesmo que seja apenas uma imagem. O comando `unsqueeze(0)` adiciona uma nova dimens√£o no in√≠cio, transformando o formato de `[3, 32, 32]` para `[1, 3, 32, 32]` (1 imagem, 3 canais, 32x32 pixels).\n",
        "\n",
        "### 3. A Camada Convolucional\n",
        "\n",
        "* **`nn.Conv2d(...)`**: Cria uma camada de convolu√ß√£o 2D. √â aqui que a m√°gica acontece:\n",
        "    * **`in_channels=3`**: Diz √† camada que a imagem de entrada tem 3 canais de cor (Vermelho, Verde, Azul).\n",
        "    * **`out_channels=16`**: Define que queremos aplicar **16 filtros diferentes**. Cada filtro vai procurar um padr√£o visual espec√≠fico e gerar o seu pr√≥prio \"mapa de caracter√≠sticas\" (feature map).\n",
        "    * **`kernel_size=3`**: O tamanho do filtro. √â uma matriz de 3x3 pixels que vai deslizar por toda a imagem fazendo c√°lculos matem√°ticos.\n",
        "\n",
        "### 4. Extra√ß√£o e Resultados\n",
        "* **`output = conv_layer(image_batch)`**: Passa a imagem pela camada convolucional (sem calcular gradientes com `torch.no_grad()`, pois √© apenas um teste).\n",
        "* **O Resultado (`output.shape`)**: O formato final gerado ser√° `[1, 16, 30, 30]`.\n",
        "    * O `1` √© o lote.\n",
        "    * O `16` s√£o os novos mapas gerados (um para cada filtro).\n",
        "    * O tamanho caiu de `32x32` para `30x30` porque um filtro 3x3 \"perde\" as bordas da imagem ao deslizar sobre ela (j√° que n√£o adicionamos preenchimento extra, ou *padding*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30LM62HXQUzj",
        "outputId": "bed58d4d-022f-4880-b669-2046579d7077"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- CNN CIFAR-10 TEST ---\n",
            "Original Image Shape: torch.Size([3, 32, 32])\n",
            "After Conv Layer (16 filters): torch.Size([1, 16, 30, 30])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Source\\Repos\\obimelo\\Curso-MachineLearning-FCUP\\.venv\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
            "  entry = pickle.load(f, encoding=\"latin1\")\n"
          ]
        }
      ],
      "source": [
        "#Extracting 16 spatial feature maps from a color image.\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "image, label = test_set[0]\n",
        "image_batch = image.unsqueeze(0)\n",
        "\n",
        "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = conv_layer(image_batch)\n",
        "\n",
        "print(\"--- CNN CIFAR-10 TEST ---\")\n",
        "print(f\"Original Image Shape: {image.shape}\")\n",
        "print(f\"After Conv Layer (16 filters): {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXw-Al0Aqxse"
      },
      "source": [
        "# üìà Previs√£o de S√©ries Temporais com Redes Recorrentes (LSTM)\n",
        "\n",
        "Este bloco de c√≥digo demonstra como usar uma arquitetura **LSTM** (Long Short-Term Memory) para prever o pr√≥ximo passo ($t+1$) de uma s√©rie temporal cont√≠nua com base em um hist√≥rico recente.\n",
        "\n",
        "\n",
        "\n",
        "### 1. Criando os Dados (A Onda Senoidal)\n",
        "\n",
        "* **`torch.linspace(0, 50, 500)`**: Cria um eixo de tempo com 500 pontos, distribu√≠dos do 0 ao 50.\n",
        "* **`series = torch.sin(t)`**: Aplica a fun√ß√£o matem√°tica seno sobre esses pontos. Isso gera dados sequenciais perfeitos para testarmos a capacidade de mem√≥ria da rede.\n",
        "\n",
        "### 2. A Janela Deslizante (Sliding Window)\n",
        "\n",
        "* **`window_size = 10`**: O modelo usar√° os √∫ltimos 10 passos de tempo para prever o 11¬∫.\n",
        "* **`test_window = ...view(1, window_size, 1)`**: O PyTorch exige que os dados para LSTMs tenham o formato exato tridimensional de `[Batch, Sequence, Feature]`. Aqui n√≥s moldamos nossa janela para:\n",
        "    * **Batch (1):** Um lote por vez.\n",
        "    * **Sequence (10):** Os 10 passos temporais.\n",
        "    * **Feature (1):** Um √∫nico valor por passo (a amplitude da onda).\n",
        "\n",
        "### 3. Construindo a Arquitetura\n",
        "* **`nn.LSTM(...)`**: A camada central do modelo.\n",
        "    * `input_size=1`: Recebe um valor por instante de tempo.\n",
        "    * `hidden_size=32`: A LSTM processa os dados e cria uma \"mem√≥ria oculta\" com 32 dimens√µes.\n",
        "    * `batch_first=True`: Indica que a primeira dimens√£o do nosso tensor √© o tamanho do lote (*Batch*).\n",
        "* **`nn.Linear(32, 1)`**: A camada de sa√≠da. Ela pega a mem√≥ria complexa de 32 dimens√µes gerada pela LSTM e a decodifica em 1 √∫nico n√∫mero (a nossa previs√£o para o futuro).\n",
        "\n",
        "### 4. Executando a Previs√£o (Infer√™ncia)\n",
        "* **`with torch.no_grad():`**: Desliga o motor de aprendizado do PyTorch (c√°lculo de gradientes) para economizar mem√≥ria durante o teste.\n",
        "* **`out, _ = lstm(test_window)`**: Passamos a janela de 10 passos pela LSTM. Ela retorna o hist√≥rico de sa√≠das (`out`).\n",
        "* **`prediction = fc(out[:, -1, :])`**: Este √© o truque principal! N√≥s fatiamos o tensor para pegar apenas a sa√≠da do **√∫ltimo passo** (`-1`), pois ele cont√©m a mem√≥ria acumulada de toda a sequ√™ncia lida. Em seguida, passamos isso pela camada Linear para obter o valor final.\n",
        "\n",
        "> **üí° Nota:** O valor previsto (`Model Prediction`) ser√° aleat√≥rio comparado ao valor real (`Actual Next Value`) porque as camadas foram inicializadas com pesos rand√¥micos e o modelo ainda n√£o foi treinado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qfyu54FCQwrJ",
        "outputId": "fe8acd2c-b465-42bc-f3cc-1d07c59030fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- RNN SINE WAVE TEST ---\n",
            "Input Sequence (first 3): [0.0, 0.10003281384706497, 0.19906212389469147]\n",
            "Actual Next Value: 0.8426\n",
            "Model Prediction: -0.1149\n"
          ]
        }
      ],
      "source": [
        "# Predicting t+1 based on a window of 10 previous steps.\n",
        "t = torch.linspace(0, 50, 500)\n",
        "series = torch.sin(t)\n",
        "\n",
        "window_size = 10\n",
        "test_window = series[:window_size].view(1, window_size, 1) # [Batch, Seq, Feature]\n",
        "\n",
        "lstm = nn.LSTM(input_size=1, hidden_size=32, batch_first=True)\n",
        "fc = nn.Linear(32, 1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out, _ = lstm(test_window)\n",
        "    prediction = fc(out[:, -1, :])\n",
        "\n",
        "print(\"\\n--- RNN SINE WAVE TEST ---\")\n",
        "print(f\"Input Sequence (first 3): {test_window.flatten()[:3].tolist()}\")\n",
        "print(f\"Actual Next Value: {series[window_size].item():.4f}\")\n",
        "print(f\"Model Prediction: {prediction.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs6W9n85sb4d"
      },
      "source": [
        "# Processamento de Texto com Transformers (Mecanismo de Aten√ß√£o)\n",
        "\n",
        "Este bloco de c√≥digo demonstra como usar uma camada de **Transformer** para criar *embeddings* (vetores matem√°ticos) cientes do contexto para uma frase. Diferente de modelos mais antigos, o Transformer olha para todas as palavras da frase ao mesmo tempo para entender como elas se relacionam.\n",
        "\n",
        "### 1. Tokeniza√ß√£o e Vocabul√°rio\n",
        "* **`sentence` e `vocab`**: Computadores n√£o leem texto, eles leem n√∫meros. Aqui criamos um pequeno dicion√°rio (vocabul√°rio) que mapeia cada palavra da nossa frase para um n√∫mero inteiro (ID). Por exemplo, \"Deep\" vira `0`, \"learning\" vira `1`.\n",
        "* **`tokens = torch.tensor(...).unsqueeze(1)`**: Converte a frase em um tensor de IDs. O `unsqueeze(1)` formata os dados no formato padr√£o que o PyTorch espera para Transformers que n√£o usam `batch_first=True`: `[Sequence Length, Batch]`. O resultado √© `[4, 1]` (4 palavras, 1 lote).\n",
        "\n",
        "### 2. A Camada de Embedding (Vetores de Palavras)\n",
        "\n",
        "* **`embed = nn.Embedding(num_embeddings=10, embedding_dim=16)`**: Esta camada transforma os IDs inteiros (0, 1, 2, 3) em vetores densos com n√∫meros decimais.\n",
        "    * `num_embeddings=10`: O tamanho m√°ximo do nosso vocabul√°rio (arbitr√°rio aqui, mas suficiente para nossas 4 palavras).\n",
        "    * `embedding_dim=16`: Cada palavra ser√° representada por uma lista de 16 n√∫meros.\n",
        "\n",
        "### 3. A Camada Transformer (Self-Attention)\n",
        "\n",
        "* **`tf_layer = nn.TransformerEncoderLayer(...)`**: O cora√ß√£o do modelo.\n",
        "    * `d_model=16`: O tamanho da entrada (precisa bater exatamente com os 16 do embedding).\n",
        "    * `nhead=4`: Define 4 \"cabe√ßas de aten√ß√£o\" (*Multi-Head Attention*). Isso significa que a rede tem 4 formas diferentes e simult√¢neas de analisar como as palavras da frase se relacionam umas com as outras (ex: uma cabe√ßa pode focar na gram√°tica, outra no sujeito da a√ß√£o).\n",
        "\n",
        "### 4. Processamento (Infer√™ncia)\n",
        "* **`embedded = embed(tokens)`**: Converte os IDs em vetores isolados. O formato agora √© `[4, 1, 16]`.\n",
        "* **`output = tf_layer(embedded)`**: Passa os vetores pelo Transformer.\n",
        "* **O Grande Segredo**: O formato de sa√≠da continua sendo `[4, 1, 16]`, mas os n√∫meros l√° dentro mudaram completamente. Antes de passar pelo Transformer, o vetor da palavra \"powerful\" significava apenas \"poderoso\". Ap√≥s o Transformer, o vetor da palavra \"powerful\" foi modificado matematicamente para conter o **contexto** de que estamos falando sobre \"Deep learning\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2eMtZSGQ9On",
        "outputId": "864bf26d-939f-4116-e29f-6ec609248456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- TRANSFORMER TEXT TEST ---\n",
            "Input Sentence: 'Deep learning is powerful'\n",
            "Token IDs: [0, 1, 2, 3]\n",
            "Output Context Vectors Shape: torch.Size([4, 1, 16])\n"
          ]
        }
      ],
      "source": [
        "# Creating context-aware embeddings for a 4-word sentence.\n",
        "sentence = \"Deep learning is powerful\"\n",
        "vocab = {\"Deep\": 0, \"learning\": 1, \"is\": 2, \"powerful\": 3}\n",
        "tokens = torch.tensor([vocab[w] for w in sentence.split()]).unsqueeze(1) # [Seq_Len, Batch]\n",
        "\n",
        "embed = nn.Embedding(num_embeddings=10, embedding_dim=16)\n",
        "tf_layer = nn.TransformerEncoderLayer(d_model=16, nhead=4)\n",
        "\n",
        "with torch.no_grad():\n",
        "    embedded = embed(tokens)\n",
        "    output = tf_layer(embedded)\n",
        "\n",
        "print(\"\\n--- TRANSFORMER TEXT TEST ---\")\n",
        "print(f\"Input Sentence: '{sentence}'\")\n",
        "print(f\"Token IDs: {tokens.flatten().tolist()}\")\n",
        "print(f\"Output Context Vectors Shape: {output.shape}\") # [4, 1, 16]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3NhuW65vwrJ"
      },
      "source": [
        "# Compress√£o e Reconstru√ß√£o de Imagens com Autoencoders\n",
        "\n",
        "Este bloco demonstra o funcionamento de um **Autoencoder**, uma rede neural dividida em duas partes: um **Encoder** (que comprime os dados) e um **Decoder** (que tenta reconstruir os dados originais a partir da vers√£o comprimida). √â a base para redu√ß√£o de dimensionalidade e gera√ß√£o de novas imagens.\n",
        "\n",
        "\n",
        "\n",
        "### 1. Preparando a Imagem Original\n",
        "* **`mnist_test = datasets.MNIST(...)`**: Carregamos novamente o dataset MNIST (d√≠gitos escritos √† m√£o).\n",
        "* **`real_digit, _ = mnist_test[0]`**: Pegamos a primeira imagem do teste (que costuma ser um n√∫mero '7'). Ignoramos o r√≥tulo (`_`) porque Autoencoders s√£o modelos n√£o-supervisionados; eles n√£o se importam com a classe da imagem, apenas com os pixels dela.\n",
        "* **`real_digit.view(-1, 784)`**: Achatamos a imagem de 28x28 pixels para um vetor de 784 n√∫meros, preparando-a para as camadas lineares. O `-1` diz ao PyTorch para deduzir automaticamente o tamanho do lote (*batch*).\n",
        "\n",
        "### 2. O Codificador (Encoder)\n",
        "* **`encoder = nn.Linear(784, 20)`**: Esta √© a fase de compress√£o (o \"gargalo\" da rede). Pegamos os 784 pixels originais e os for√ßamos a caber em um vetor de apenas 20 n√∫meros. Essa representa√ß√£o comprimida √© chamada de **Espa√ßo Latente** (*Latent Space*).\n",
        "\n",
        "### 3. O Decodificador (Decoder)\n",
        "* **`decoder = nn.Sequential(...)`**: Esta rede faz o caminho inverso.\n",
        "    * **`nn.Linear(20, 400)` e `nn.ReLU()`**: Pega o vetor comprimido de 20 n√∫meros e come√ßa a expandi-lo para 400 neur√¥nios.\n",
        "    * **`nn.Linear(400, 784)`**: Expande novamente para os 784 pixels originais.\n",
        "    * **`nn.Sigmoid()`**: Como os pixels originais da imagem foram convertidos pelo `ToTensor()` para valores entre 0 e 1, a fun√ß√£o Sigmoid garante que a sa√≠da da nossa rede tamb√©m fique restrita a esse mesmo intervalo.\n",
        "\n",
        "### 4. Executando a Compress√£o e Reconstru√ß√£o\n",
        "* **`latent_vector = encoder(...)`**: Transforma o d√≠gito '7' em um resumo de 20 n√∫meros.\n",
        "* **`reconstruction = decoder(latent_vector).view(1, 28, 28)`**: Pega o resumo de 20 n√∫meros, tenta reconstruir os 784 pixels e usa o `view` para remodelar o vetor de volta ao formato de grade 2D (imagem de 28x28).\n",
        "* Os **prints** finais somam os valores dos pixels para comparar a imagem original com a reconstru√≠da.\n",
        "\n",
        "> **üí° Nota:** Como o modelo n√£o foi treinado, o \"resumo\" criado pelo Encoder √© aleat√≥rio, e a reconstru√ß√£o feita pelo Decoder parecer√° apenas ru√≠do (chuvisco de TV). Para que ele aprenda a reconstruir o '7' perfeitamente, precisamos trein√°-lo comparando a sa√≠da com a entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjXImd3CugHb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oti6Ze5NRASU",
        "outputId": "67038b9f-253f-4143-e7f1-ab4c3498cc4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- VAE MNIST RECONSTRUCTION TEST ---\n",
            "Input image pixels: 72.37\n",
            "Reconstructed pixels: 392.77\n"
          ]
        }
      ],
      "source": [
        "# Compressing a digit to 20 numbers and rebuilding it.\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "real_digit, _ = mnist_test[0] # A '7'\n",
        "real_digit_flat = real_digit.view(-1, 784)\n",
        "\n",
        "encoder = nn.Linear(784, 20)\n",
        "decoder = nn.Sequential(nn.Linear(20, 400), nn.ReLU(), nn.Linear(400, 784), nn.Sigmoid())\n",
        "\n",
        "with torch.no_grad():\n",
        "    latent_vector = encoder(real_digit_flat)\n",
        "    reconstruction = decoder(latent_vector).view(1, 28, 28)\n",
        "\n",
        "print(\"\\n--- VAE MNIST RECONSTRUCTION TEST ---\")\n",
        "print(f\"Input image pixels: {real_digit.sum().item():.2f}\")\n",
        "print(f\"Reconstructed pixels: {reconstruction.sum().item():.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
